{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GAMMA Challenge Subtask 3 - Official Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Link for GAMMA:\n",
    "\n",
    "\tMICCAI2021 Contest - GAMMA: https://aistudio.baidu.com/aistudio/competition/detail/90\n",
    "\n",
    "Challenge Description:\n",
    "\n",
    "\tThe GAMMA Challenge is an international ophthalmology competition held by Baidu at the MICCAI2021 seminar OMIA8. MICCAI is a comprehensive academic conference in the fields of medical image computing and computer assisted intervention, and is the top conference in these fields. OMIA is an Ophthalmic Medical Image Analysis seminar organized by Baidu at the MICCAI conference, which has been held for eight sessions so far.\n",
    "\n",
    "    The GAMMA Challenge focused on glaucoma analysis in multimodal images and consisted of three sub-tasks:  \n",
    "1) glaucoma grading, 2) macular fovea localization, 3) optic disc and cup segmentation.  \n",
    "    \n",
    "Task Description of this baseline\n",
    "\n",
    "\tThis baseline corresponds to Task 2 of the GAMMA Challenge, which is to segment the optic disc and optic cup in 2D color fundus images.\n",
    "    \n",
    "Dataset Description\n",
    "\n",
    "    The dataset used for this baseline is 2D colour fundus images released in GAMMA. Users can obtain the corresponding datasets by signing up for the GAMMA challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### remove the extraneous files in the data folder\n",
    "\n",
    "!rm */.DS_Store\n",
    "!rm */*/.DS_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### import the necessary packages\n",
    "\n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances \n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### set the parameters in your framework\n",
    "\n",
    "images_file = ''  # the path to the training data\n",
    "gt_file = 'Disc_Cup_Mask/'\n",
    "test_file = ''  # the path to the testing data\n",
    "image_size = 256 # the image size to the network (image_size, image_size, 3)\n",
    "val_ratio = 0.2  # the ratio of train/validation splitition\n",
    "BATCH_SIZE = 8 # batch size\n",
    "iters = 3000 # training iteration\n",
    "optimizer_type = 'adam' # the optimizer, can be set as SGD, RMSprop,...\n",
    "num_workers = 4 # Number of workers used to load data\n",
    "init_lr = 1e-3 # the initial learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train / Val splitition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### divide the training image and the verification image from the training set \n",
    "\n",
    "filelists = os.listdir(images_file)\n",
    "train_filelists, val_filelists = train_test_split(filelists, test_size = val_ratio,random_state = 42)\n",
    "print(\"Total Nums: {}, train: {}, val: {}\".format(len(filelists), len(train_filelists), len(val_filelists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### load the fundus images from the data folder, \n",
    "### and extract the corresponding ground truth to generate training samples\n",
    "\n",
    "class FundusDataset(Dataset):\n",
    "    def __init__(self, image_file, gt_path=None, filelists=None,  mode='train'):\n",
    "        super(FundusDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.image_path = image_file\n",
    "        image_idxs = os.listdir(self.image_path) # 0001, fundus_img in the folder 0001\n",
    "        self.gt_path = gt_path\n",
    "\n",
    "        self.file_list = [image_idxs[i] for i in range(len(image_idxs))]        \n",
    "        \n",
    "        if filelists is not None:\n",
    "            self.file_list = [item for item in self.file_list if item in filelists] \n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        real_index = self.file_list[idx]\n",
    "        fundus_img_path = os.path.join(self.image_path, real_index, real_index + '.jpg')\n",
    "        fundus_img = cv2.imread(fundus_img_path)[:, :, ::-1] # BGR -> RGB        \n",
    "        h,w,c = fundus_img.shape\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            gt_tmp_path = os.path.join(self.gt_path, real_index + '.png')\n",
    "            gt_img = cv2.imread(gt_tmp_path)\n",
    "\n",
    "            ### In the ground truth, a pixel value of 0 is the optic cup (class 0), \n",
    "            ### a pixel value of 128 is the optic disc (class 1), \n",
    "            ### and a pixel value of 255 is the background (class 2).\n",
    "            \n",
    "            gt_img[gt_img == 128] = 1\n",
    "            gt_img[gt_img == 255] = 2\n",
    "            gt_img = cv2.resize(gt_img,(image_size, image_size))\n",
    "            gt_img = gt_img[:,:,1]\n",
    "            # print('gt shape', gt_img.shape)           \n",
    "\n",
    "        fundus_re = cv2.resize(fundus_img,(image_size, image_size))\n",
    "        img = fundus_re.transpose(2, 0, 1) # H, W, C -> C, H, W\n",
    "        # print(img.shape)\n",
    "        # img = fundus_re.astype(np.float32)\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            ### During the testing process, \n",
    "            ### the sample returns fundus image, sample name, \n",
    "            ### height and width of the original image\n",
    "\n",
    "            return img, real_index, h, w\n",
    "        if self.mode == 'train':\n",
    "            ### During the training process,\n",
    "            ### the sample returns fundus image and its corresponding ground truth\n",
    "            \n",
    "            return img, gt_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### generate a _train and a _val Dataset for presenting images in the training dataset\n",
    "\n",
    "_train = FundusDataset(image_file = images_file, \n",
    "                        gt_path = gt_file)\n",
    "\n",
    "_val = FundusDataset(image_file = images_file, \n",
    "                        gt_path = gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### present five fundus images and corresponding ground truths in the _train Dataset\n",
    "### there are three classes in the ground truth: 0-optic cup, 1-optic disc, 2-background\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "for i in range(5):\n",
    "    fundus_img, label = _train.__getitem__(i)\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(fundus_img.transpose(1,2,0))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2,5,i+6)\n",
    "    plt.imshow(label)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### present five fundus images and corresponding ground truths in the _val Dataset\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "for i in range(5):\n",
    "    fundus_img, label = _val.__getitem__(i)\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(fundus_img.transpose(1,2,0))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2,5,i+6)\n",
    "    plt.imshow(label)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This network is UNet.\n",
    "U-NET is a U-shaped network structure, which can be seen as two large stages. The image is first sampled by the Encoder to obtain the high-level semantic feature map, and then sampled by the Decoder to restore the feature map to the resolution of the original image.\n",
    "The details of the codes can be seen at https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/cv_case/image_segmentation/image_segmentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SeparableConv2D(nn.Layer):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 dilation=1, \n",
    "                 groups=None, \n",
    "                 weight_attr=None, \n",
    "                 bias_attr=None, \n",
    "                 data_format=\"NCHW\"):\n",
    "        super(SeparableConv2D, self).__init__()\n",
    "\n",
    "        self._padding = padding\n",
    "        self._stride = stride\n",
    "        self._dilation = dilation\n",
    "        self._in_channels = in_channels\n",
    "        self._data_format = data_format\n",
    "\n",
    "        # 第一次卷积参数，没有偏置参数\n",
    "        filter_shape = [in_channels, 1] + self.convert_to_list(kernel_size, 2, 'kernel_size')\n",
    "        self.weight_conv = self.create_parameter(shape=filter_shape, attr=weight_attr)\n",
    "\n",
    "        # 第二次卷积参数\n",
    "        filter_shape = [out_channels, in_channels] + self.convert_to_list(1, 2, 'kernel_size')\n",
    "        self.weight_pointwise = self.create_parameter(shape=filter_shape, attr=weight_attr)\n",
    "        self.bias_pointwise = self.create_parameter(shape=[out_channels], \n",
    "                                                    attr=bias_attr, \n",
    "                                                    is_bias=True)\n",
    "    \n",
    "    def convert_to_list(self, value, n, name, dtype=np.int):\n",
    "        if isinstance(value, dtype):\n",
    "            return [value, ] * n\n",
    "        else:\n",
    "            try:\n",
    "                value_list = list(value)\n",
    "            except TypeError:\n",
    "                raise ValueError(\"The \" + name +\n",
    "                                \"'s type must be list or tuple. Received: \" + str(\n",
    "                                    value))\n",
    "            if len(value_list) != n:\n",
    "                raise ValueError(\"The \" + name + \"'s length must be \" + str(n) +\n",
    "                                \". Received: \" + str(value))\n",
    "            for single_value in value_list:\n",
    "                try:\n",
    "                    dtype(single_value)\n",
    "                except (ValueError, TypeError):\n",
    "                    raise ValueError(\n",
    "                        \"The \" + name + \"'s type must be a list or tuple of \" + str(\n",
    "                            n) + \" \" + str(dtype) + \" . Received: \" + str(\n",
    "                                value) + \" \"\n",
    "                        \"including element \" + str(single_value) + \" of type\" + \" \"\n",
    "                        + str(type(single_value)))\n",
    "            return value_list\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        conv_out = F.conv2d(inputs, \n",
    "                            self.weight_conv, \n",
    "                            padding=self._padding,\n",
    "                            stride=self._stride,\n",
    "                            dilation=self._dilation,\n",
    "                            groups=self._in_channels,\n",
    "                            data_format=self._data_format)\n",
    "        \n",
    "        out = F.conv2d(conv_out,\n",
    "                       self.weight_pointwise,\n",
    "                       bias=self.bias_pointwise,\n",
    "                       padding=0,\n",
    "                       stride=1,\n",
    "                       dilation=1,\n",
    "                       groups=1,\n",
    "                       data_format=self._data_format)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.relus = nn.LayerList(\n",
    "            [nn.ReLU() for i in range(2)])\n",
    "        self.separable_conv_01 = SeparableConv2D(in_channels, \n",
    "                                                 out_channels, \n",
    "                                                 kernel_size=3, \n",
    "                                                 padding='same')\n",
    "        self.bns = nn.LayerList(\n",
    "            [nn.BatchNorm2D(out_channels) for i in range(2)])\n",
    "        \n",
    "        self.separable_conv_02 = SeparableConv2D(out_channels, \n",
    "                                                 out_channels, \n",
    "                                                 kernel_size=3, \n",
    "                                                 padding='same')\n",
    "        self.pool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "        self.residual_conv = nn.Conv2D(in_channels, \n",
    "                                        out_channels, \n",
    "                                        kernel_size=1, \n",
    "                                        stride=2, \n",
    "                                        padding='same')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        previous_block_activation = inputs\n",
    "        \n",
    "        y = self.relus[0](inputs)\n",
    "        y = self.separable_conv_01(y)\n",
    "        y = self.bns[0](y)\n",
    "        y = self.relus[1](y)\n",
    "        y = self.separable_conv_02(y)\n",
    "        y = self.bns[1](y)\n",
    "        y = self.pool(y)\n",
    "        \n",
    "        residual = self.residual_conv(previous_block_activation)\n",
    "        y = paddle.add(y, residual)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.relus = nn.LayerList(\n",
    "            [nn.ReLU() for i in range(2)])\n",
    "        self.conv_transpose_01 = nn.Conv2DTranspose(in_channels, \n",
    "                                                           out_channels, \n",
    "                                                           kernel_size=3, \n",
    "                                                           padding=1)\n",
    "        self.conv_transpose_02 = nn.Conv2DTranspose(out_channels, \n",
    "                                                           out_channels, \n",
    "                                                           kernel_size=3, \n",
    "                                                           padding=1)\n",
    "        self.bns = nn.LayerList(\n",
    "            [nn.BatchNorm2D(out_channels) for i in range(2)]\n",
    "        )\n",
    "        self.upsamples = nn.LayerList(\n",
    "            [nn.Upsample(scale_factor=2.0) for i in range(2)]\n",
    "        )\n",
    "        self.residual_conv = nn.Conv2D(in_channels, \n",
    "                                        out_channels, \n",
    "                                        kernel_size=1, \n",
    "                                        padding='same')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        previous_block_activation = inputs\n",
    "\n",
    "        y = self.relus[0](inputs)\n",
    "        y = self.conv_transpose_01(y)\n",
    "        y = self.bns[0](y)\n",
    "        y = self.relus[1](y)\n",
    "        y = self.conv_transpose_02(y)\n",
    "        y = self.bns[1](y)\n",
    "        y = self.upsamples[0](y)\n",
    "        \n",
    "        residual = self.upsamples[1](previous_block_activation)\n",
    "        residual = self.residual_conv(residual)\n",
    "        \n",
    "        y = paddle.add(y, residual)\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cup_disc_UNet(nn.Layer):\n",
    "    def __init__(self, num_classes):\n",
    "        super(cup_disc_UNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2D(3, 32, \n",
    "                                kernel_size=3,\n",
    "                                stride=2,\n",
    "                                padding='same')\n",
    "        self.bn = nn.BatchNorm2D(32)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        in_channels = 32\n",
    "        self.encoders = []\n",
    "        self.encoder_list = [64, 128, 256]\n",
    "        self.decoder_list = [256, 128, 64, 32]\n",
    "\n",
    "        # 根据下采样个数和配置循环定义子Layer，避免重复写一样的程序\n",
    "        for out_channels in self.encoder_list:\n",
    "            block = self.add_sublayer('encoder_{}'.format(out_channels),\n",
    "                                      Encoder(in_channels, out_channels))\n",
    "            self.encoders.append(block)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        # 根据上采样个数和配置循环定义子Layer，避免重复写一样的程序\n",
    "        for out_channels in self.decoder_list:\n",
    "            block = self.add_sublayer('decoder_{}'.format(out_channels), \n",
    "                                      Decoder(in_channels, out_channels))\n",
    "            self.decoders.append(block)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.output_conv = nn.Conv2D(in_channels, \n",
    "                                            num_classes, \n",
    "                                            kernel_size=3, \n",
    "                                            padding='same')\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        y = self.conv_1(inputs)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            y = encoder(y)\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            y = decoder(y)\n",
    "        \n",
    "        y = self.output_conv(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### we use DICE metric to validate the predicted results \n",
    "### The detailed introduction of DICE coefficient \n",
    "### can be found at https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "\n",
    "class DiceLoss(nn.Layer):\n",
    "    \"\"\"\n",
    "    Implements the dice loss function.\n",
    "    Args:\n",
    "        ignore_index (int64): Specifies a target value that is ignored\n",
    "            and does not contribute to the input gradient. Default ``255``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ignore_index=2):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        if len(labels.shape) != len(logits.shape):\n",
    "            labels = paddle.unsqueeze(labels, 1)\n",
    "        num_classes = logits.shape[1]\n",
    "        mask = (labels != self.ignore_index)\n",
    "        logits = logits * mask\n",
    "        labels = paddle.cast(labels, dtype='int32')\n",
    "        single_label_lists = []\n",
    "        for c in range(num_classes):\n",
    "            single_label = paddle.cast((labels == c), dtype='int32')\n",
    "            single_label = paddle.squeeze(single_label, axis=1)\n",
    "            single_label_lists.append(single_label)\n",
    "        labels_one_hot = paddle.stack(tuple(single_label_lists), axis=1)\n",
    "        logits = F.softmax(logits, axis=1)\n",
    "        labels_one_hot = paddle.cast(labels_one_hot, dtype='float32')\n",
    "        dims = (0,) + tuple(range(2, labels.ndimension()))\n",
    "        intersection = paddle.sum(logits * labels_one_hot, dims)\n",
    "        cardinality = paddle.sum(logits + labels_one_hot, dims)\n",
    "        dice_loss = (2. * intersection / (cardinality + self.eps)).mean()\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Training function\n",
    "\n",
    "def train(model, iters, train_dataloader, val_dataloader, optimizer, criterion, metric, log_interval, evl_interval):\n",
    "    iter = 0\n",
    "    model.train()\n",
    "    avg_loss_list = []\n",
    "    avg_dice_list = []\n",
    "    best_dice = 0.\n",
    "    while iter < iters:\n",
    "        for data in train_dataloader:\n",
    "            iter += 1\n",
    "            if iter > iters:\n",
    "                break\n",
    "            fundus_img = (data[0]/255.).astype(\"float32\")\n",
    "            gt_label = (data[1]).astype(\"int64\")\n",
    "            # print('label shape: ', gt_label.shape)\n",
    "            logits = model(fundus_img)\n",
    "            # print('logits shape: ', logits.shape)\n",
    "            loss = criterion(logits, gt_label)\n",
    "            # print('loss: ',loss)\n",
    "            dice = metric(logits, gt_label) \n",
    "            # print('dice: ', dice)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.clear_gradients()\n",
    "            avg_loss_list.append(loss.numpy()[0])\n",
    "            avg_dice_list.append(dice.numpy()[0]) \n",
    "\n",
    "            if iter % log_interval == 0:\n",
    "                avg_loss = np.array(avg_loss_list).mean()\n",
    "                avg_dice = np.array(avg_dice_list).mean()\n",
    "                avg_loss_list = []\n",
    "                avg_dice_list = []\n",
    "                print(\"[TRAIN] iter={}/{} avg_loss={:.4f} avg_dice={:.4f}\".format(iter, iters, avg_loss, avg_dice))\n",
    "\n",
    "            if iter % evl_interval == 0:\n",
    "                avg_loss, avg_dice = val(model, val_dataloader)\n",
    "                print(\"[EVAL] iter={}/{} avg_loss={:.4f} dice={:.4f}\".format(iter, iters, avg_loss, avg_dice))\n",
    "                if avg_dice >= best_dice:\n",
    "                    best_dice = avg_dice\n",
    "                    paddle.save(model.state_dict(),\n",
    "                                os.path.join(\"best_model_{:.4f}\".format(best_dice), 'model.pdparams'))\n",
    "                model.train()\n",
    "\n",
    "### validation function\n",
    "\n",
    "def val(model, val_dataloader):\n",
    "    model.eval()\n",
    "    avg_loss_list = []\n",
    "    avg_dice_list = []\n",
    "    with paddle.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            fundus_img = (data[0] / 255.).astype(\"float32\")\n",
    "            gt_label = (data[1]).astype(\"int64\")\n",
    "\n",
    "            pred = model(fundus_img)\n",
    "            loss = criterion(pred, gt_label)\n",
    "            dice = metric (pred, gt_label)  \n",
    "\n",
    "            avg_loss_list.append(loss.numpy()[0])\n",
    "            avg_dice_list.append(dice.numpy()[0])\n",
    "\n",
    "    avg_loss = np.array(avg_loss_list).mean()\n",
    "    avg_dice = np.array(avg_dice_list).mean()\n",
    "\n",
    "    return avg_loss, avg_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### generate training Dataset and validation Dataset \n",
    "\n",
    "train_dataset = FundusDataset(image_file = images_file, \n",
    "                        gt_path = gt_file,\n",
    "                        filelists=train_filelists)\n",
    "\n",
    "val_dataset = FundusDataset(image_file = images_file, \n",
    "                        gt_path = gt_file,\n",
    "                        filelists=val_filelists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the samples\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=paddle.io.DistributedBatchSampler(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False),\n",
    "    num_workers=num_workers,\n",
    "    return_list=True,\n",
    "    use_shared_memory=False\n",
    ")\n",
    "\n",
    "val_loader = paddle.io.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=paddle.io.DistributedBatchSampler(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False),\n",
    "    num_workers=num_workers,\n",
    "    return_list=True,\n",
    "    use_shared_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Model code was used to generate Model instance, and Optimizer, loss function, \n",
    "### evaluation index and other information were defined for subsequent training.\n",
    "\n",
    "model = cup_disc_UNet(num_classes=3)\n",
    "\n",
    "### The SUMMARY interface provided by the paddlepaddle is called to visualize the constructed model, \n",
    "### which is convenient to view and confirm the model structure and parameter information.\n",
    "# paddle.Model(model).summary((-1,3,256,256)) \n",
    "\n",
    "if optimizer_type == \"adam\":\n",
    "    optimizer = paddle.optimizer.Adam(init_lr, parameters=model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(axis=1)\n",
    "metric = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### training process\n",
    "\n",
    "train(model, iters, train_loader, val_loader, optimizer, criterion, metric, log_interval=50, evl_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### inference(testing) process, load the model parameters\n",
    "\n",
    "best_model_path = \"./best_model_0.1794/model.pdparams\"\n",
    "model = cup_disc_UNet(num_classes = 3)\n",
    "para_state_dict = paddle.load(best_model_path)\n",
    "model.set_state_dict(para_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### gerenate the test Dataset\n",
    "\n",
    "test_dataset = FundusDataset(image_file = test_file, \n",
    "                            mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### The fundus images in the test dataset are segmented one by one\n",
    "### The segmentation results are resized and stored as BMP images\n",
    "\n",
    "for fundus_img, idx, h, w in test_dataset:\n",
    "    # print(idx)\n",
    "    fundus_img = fundus_img[np.newaxis, ...]\n",
    "    fundus_img = paddle.to_tensor((fundus_img / 255.).astype(\"float32\"))\n",
    "    logits = model(fundus_img)\n",
    "    pred_img = logits.numpy().argmax(1)\n",
    "    pred_gray = np.squeeze(pred_img, axis=0)\n",
    "    pred_gray = pred_gray.astype('float32')\n",
    "    # print(pred_gray.shape)\n",
    "    pred_gray[pred_gray == 1] = 128\n",
    "    pred_gray[pred_gray == 2] = 255\n",
    "    # print(pred_gray)\n",
    "    pred_ = cv2.resize(pred_gray, (w, h))\n",
    "    # print(pred_.shape)\n",
    "    cv2.imwrite('Disc_Cup_Segmentations/'+idx+'.bmp', pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    This baseline realized the segmentation of optic cup and optic disc in 2D color fundus photography, and the baseline model is U-Net.\n",
    "    Users can try other tricks on the basis of baseline, such as joint training with macular segmentation or lacalization tasks, and realizing the segmentation from coarse to fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
